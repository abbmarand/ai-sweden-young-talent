{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 1 of 8):\n",
      "         id                                               text  \\\n",
      "0  30060320  Cédric Gerbehaye (born 1977) is a Belgian jour...   \n",
      "1  30060327  The West Virginia Capitol Complex is a histori...   \n",
      "2  30060339  It's Real may refer to: * It's Real (K-Ci & Jo...   \n",
      "3  30060356  The 2011 Blancpain Endurance Series season was...   \n",
      "4  30060369  Terra Venture Partners is an Israeli venture c...   \n",
      "\n",
      "                             title  \n",
      "0                 Cédric Gerbehaye  \n",
      "1    West Virginia Capitol Complex  \n",
      "2                        It's Real  \n",
      "3  2011 Blancpain Endurance Series  \n",
      "4           Terra Venture Partners  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 2 of 8):\n",
      "           id                                               text  \\\n",
      "593  30072986  Jhonatan Longhi (born February 2, 1988) is an ...   \n",
      "594  30073006  Marko Rudić (born January 17, 1990) is an alpi...   \n",
      "595  30073009  Shady is an unincorporated community in Dougla...   \n",
      "596  30073013  140px Mary Louise Peebles, née Parmelee (1833–...   \n",
      "597  30073016  John Buttigieg may refer to: *John Buttigieg (...   \n",
      "\n",
      "                   title  \n",
      "593      Jhonatan Longhi  \n",
      "594          Marko Rudić  \n",
      "595        Shady, Oregon  \n",
      "596  Mary Louise Peebles  \n",
      "597       John Buttigieg  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 3 of 8):\n",
      "            id                                               text  \\\n",
      "1186  30080390  Ostrogothic Ravenna refers to the time period ...   \n",
      "1187  30080396  The King's American Dragoons were a British pr...   \n",
      "1188  30080403  Red Sky Music Festival was a yearly music fest...   \n",
      "1189  30080459  Ellingham Hall is an historic country house in...   \n",
      "1190  30080503  Eriogonum callistum is a rare species of wild ...   \n",
      "\n",
      "                         title  \n",
      "1186       Ostrogothic Ravenna  \n",
      "1187  King's American Dragoons  \n",
      "1188    Red Sky Music Festival  \n",
      "1189   Ellingham Hall, Norfolk  \n",
      "1190       Eriogonum callistum  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 4 of 8):\n",
      "            id                                               text  \\\n",
      "1779  30086005  The following elections occurred in the year 1...   \n",
      "1780  30086008  The following elections occurred in the year 1...   \n",
      "1781  30086010  The following elections occurred in the year 1...   \n",
      "1782  30086013  The following elections occurred in the year 1...   \n",
      "1783  30086017  The following elections occurred in the year 1...   \n",
      "\n",
      "                          title  \n",
      "1779  List of elections in 1949  \n",
      "1780  List of elections in 1950  \n",
      "1781  List of elections in 1951  \n",
      "1782  List of elections in 1952  \n",
      "1783  List of elections in 1953  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 5 of 8):\n",
      "            id                                               text  \\\n",
      "2372  30092720  The 1949–50 season was Port Vale's 38th season...   \n",
      "2373  30092732  S. Krishnamoorthy or S. Krishnamurthy (Subrama...   \n",
      "2374  30092745  King Edward Memorial Park is a public open spa...   \n",
      "2375  30092782  The Canadian Journal of Civil Engineering is a...   \n",
      "2376  30092794  The Marble Valley Bridge () is a bridge locate...   \n",
      "\n",
      "                                      title  \n",
      "2372          1949–50 Port Vale F.C. season  \n",
      "2373             Subramanian Krishnamoorthy  \n",
      "2374              King Edward Memorial Park  \n",
      "2375  Canadian Journal of Civil Engineering  \n",
      "2376                   Marble Valley Bridge  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 6 of 8):\n",
      "            id                                               text  \\\n",
      "2965  30102167  Emile Kirscht (1913–1994) was a Luxembourg pai...   \n",
      "2966  30102168  Almost, Maine is a play by John Cariani, compr...   \n",
      "2967  30102180  George Archibald Walters (30 March 1939 – Dece...   \n",
      "2968  30102181  Stigmella palmatae is a moth of the family Nep...   \n",
      "2969  30102182  Protector of the Indians () was an administrat...   \n",
      "\n",
      "                            title  \n",
      "2965                Emile Kirscht  \n",
      "2966                Almost, Maine  \n",
      "2967  George Walters (footballer)  \n",
      "2968           Stigmella palmatae  \n",
      "2969     Protector of the Indians  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 7 of 8):\n",
      "            id                                               text  \\\n",
      "3558  30108239  The People's Palace is a heritage-listed build...   \n",
      "3559  30108257  Jamie and Gladys Scott, often referred to as t...   \n",
      "3560  30108267  Henry Master Feilden (21 February 1818 – 5 Sep...   \n",
      "3561  30108275  The Greenville and Columbia Railroad was a Con...   \n",
      "3562  30108281  Map of Ethiopia showing Gambela Wentawo is one...   \n",
      "\n",
      "                                        title  \n",
      "3558                People's Palace, Brisbane  \n",
      "3559                            Scott sisters  \n",
      "3560  Henry Feilden (Conservative politician)  \n",
      "3561         Greenville and Columbia Railroad  \n",
      "3562                                  Wentawo  \n",
      "Loaded 54814a89-cfc6-4429-a44b-ef9a1f256971.json (chunk 8 of 8):\n",
      "            id                                               text  \\\n",
      "4151  30113398  Rabboni is a public artwork by American artist...   \n",
      "4152  30113425  The World Military Cross Country Championships...   \n",
      "4153  30113460  Some sources opposed to the Iranian government...   \n",
      "4154  30113494  Location of Trinity Peninsula. Utus Peak (, ) ...   \n",
      "4155  30113508  {| class=\"wikitable\" width=\"25%\" style=\"float:...   \n",
      "\n",
      "                                           title  \n",
      "4151                         Rabboni (sculpture)  \n",
      "4152  World Military Cross Country Championships  \n",
      "4153                    Plastic Keys to Paradise  \n",
      "4154                                   Utus Peak  \n",
      "4155                                Is functions  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30060320</td>\n",
       "      <td>Cédric Gerbehaye (born 1977) is a Belgian jour...</td>\n",
       "      <td>Cédric Gerbehaye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30060327</td>\n",
       "      <td>The West Virginia Capitol Complex is a histori...</td>\n",
       "      <td>West Virginia Capitol Complex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30060339</td>\n",
       "      <td>It's Real may refer to: * It's Real (K-Ci &amp; Jo...</td>\n",
       "      <td>It's Real</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30060356</td>\n",
       "      <td>The 2011 Blancpain Endurance Series season was...</td>\n",
       "      <td>2011 Blancpain Endurance Series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30060369</td>\n",
       "      <td>Terra Venture Partners is an Israeli venture c...</td>\n",
       "      <td>Terra Venture Partners</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>30121786</td>\n",
       "      <td>Devan Deangelo Downey (born September 28, 1987...</td>\n",
       "      <td>Devan Downey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4743</th>\n",
       "      <td>30121798</td>\n",
       "      <td>\"The Hand That Rocks the Wheelchair\" is the 12...</td>\n",
       "      <td>The Hand That Rocks the Wheelchair</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4744</th>\n",
       "      <td>30121817</td>\n",
       "      <td>The 1995 Supercopa Libertadores was the eighth...</td>\n",
       "      <td>1995 Supercopa Libertadores</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4745</th>\n",
       "      <td>30121823</td>\n",
       "      <td>Carl Davis (born November 16, 1973) is an Amer...</td>\n",
       "      <td>Carl Davis (boxer)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4746</th>\n",
       "      <td>30121824</td>\n",
       "      <td>Nysa or Nyssa (, flourished second half of 2nd...</td>\n",
       "      <td>Nysa (daughter of Nicomedes III of Bithynia)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                               text  \\\n",
       "0     30060320  Cédric Gerbehaye (born 1977) is a Belgian jour...   \n",
       "1     30060327  The West Virginia Capitol Complex is a histori...   \n",
       "2     30060339  It's Real may refer to: * It's Real (K-Ci & Jo...   \n",
       "3     30060356  The 2011 Blancpain Endurance Series season was...   \n",
       "4     30060369  Terra Venture Partners is an Israeli venture c...   \n",
       "...        ...                                                ...   \n",
       "4742  30121786  Devan Deangelo Downey (born September 28, 1987...   \n",
       "4743  30121798  \"The Hand That Rocks the Wheelchair\" is the 12...   \n",
       "4744  30121817  The 1995 Supercopa Libertadores was the eighth...   \n",
       "4745  30121823  Carl Davis (born November 16, 1973) is an Amer...   \n",
       "4746  30121824  Nysa or Nyssa (, flourished second half of 2nd...   \n",
       "\n",
       "                                             title  \n",
       "0                                 Cédric Gerbehaye  \n",
       "1                    West Virginia Capitol Complex  \n",
       "2                                        It's Real  \n",
       "3                  2011 Blancpain Endurance Series  \n",
       "4                           Terra Venture Partners  \n",
       "...                                            ...  \n",
       "4742                                  Devan Downey  \n",
       "4743            The Hand That Rocks the Wheelchair  \n",
       "4744                   1995 Supercopa Libertadores  \n",
       "4745                            Carl Davis (boxer)  \n",
       "4746  Nysa (daughter of Nicomedes III of Bithynia)  \n",
       "\n",
       "[4747 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "fulldata = pd.DataFrame()\n",
    "datadir = \"/home/m/dev/ai/llm/wiki_en\"\n",
    "\n",
    "for filename in os.listdir(datadir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        with open(os.path.join(datadir, filename), \"r\") as f:\n",
    "            data = json.load(f)\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Split the data into 8 parts\n",
    "            chunk_size = len(df) // 8\n",
    "            for i in range(8):\n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size if i < 7 else len(df)\n",
    "                chunk_df = df.iloc[start:end]\n",
    "                \n",
    "                fulldata = pd.concat([fulldata, chunk_df], ignore_index=True)\n",
    "                print(f\"Loaded {filename} (chunk {i+1} of 8):\")\n",
    "                print(chunk_df.head())\n",
    "            \n",
    "            break  # This break is kept to maintain the original behavior of processing only one file\n",
    "\n",
    "fulldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import math\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6, dim_feedforward=2048, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_encoder_layers)\n",
    "        \n",
    "        decoder_layers = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layers, num_decoder_layers)\n",
    "        \n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        memory = self.transformer_encoder(src, src_key_padding_mask=src_mask)\n",
    "        \n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "        tgt = self.pos_encoder(tgt)\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_key_padding_mask=tgt_mask)\n",
    "        \n",
    "        return self.output_layer(output)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class WikiDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text']\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten()\n",
    "        }\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "vocab_size = tokenizer.vocab_size\n",
    "model = TransformerModel(vocab_size)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "\n",
    "\n",
    "dataset = WikiDataset(fulldata, tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 1\n",
    "max_batches = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model.train()\n",
    "total_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, batch in enumerate(dataloader):\n",
    "        if batch_idx >= max_batches:\n",
    "            break\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        target = input_ids[:, 1:].contiguous()\n",
    "        input_ids = input_ids[:, :-1].contiguous()\n",
    "        \n",
    "        src_key_padding_mask = (~attention_mask[:, :-1].bool()).transpose(0, 1).to(device)\n",
    "        tgt_key_padding_mask = (~attention_mask[:, :-1].bool()).transpose(0, 1).to(device)\n",
    "        \n",
    "        assert src_key_padding_mask.shape == (input_ids.size(1), input_ids.size(0)), f\"src_key_padding_mask shape: {src_key_padding_mask.shape}, expected: {(input_ids.size(1), input_ids.size(0))}\"\n",
    "        assert tgt_key_padding_mask.shape == (input_ids.size(1), input_ids.size(0)), f\"tgt_key_padding_mask shape: {tgt_key_padding_mask.shape}, expected: {(input_ids.size(1), input_ids.size(0))}\"\n",
    "        \n",
    "        outputs = model(input_ids, input_ids, src_mask=src_key_padding_mask, tgt_mask=tgt_key_padding_mask)\n",
    "        loss = criterion(outputs.view(-1, vocab_size), target.view(-1))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{max_batches}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / max_batches\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training completed.\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'wiki_transformer_model.pth')\n",
    "print(\"Model saved as 'wiki_transformer_model.pth'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Loading the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_200907/1059042840.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('wiki_transformer_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully\n",
      "\n",
      "Step 2: Initializing tokenizer\n",
      "Tokenizer initialized\n",
      "\n",
      "Step 4: Running a prompt\n",
      "Prompt: The\n",
      "\n",
      "Step 3: Generating text for prompt: 'The'\n",
      "  Generating token 1/10\n",
      "  Generating token 2/10\n",
      "  Generating token 3/10\n",
      "  Generating token 4/10\n",
      "  Generating token 5/10\n",
      "  Generating token 6/10\n",
      "  Generating token 7/10\n",
      "  Generating token 8/10\n",
      "  Generating token 9/10\n",
      "  Generating token 10/10\n",
      "Text generation completed\n",
      "Generated text: the..........\n",
      "\n",
      "Step 5: Process completed\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "print(\"Step 1: Loading the model\")\n",
    "model = TransformerModel(vocab_size)\n",
    "model.load_state_dict(torch.load('wiki_transformer_model.pth'))\n",
    "model.eval()\n",
    "print(\"Model loaded successfully\")\n",
    "\n",
    "# Initialize tokenizer\n",
    "print(\"\\nStep 2: Initializing tokenizer\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(\"Tokenizer initialized\")\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt, max_length=10):\n",
    "    print(f\"\\nStep 3: Generating text for prompt: '{prompt}'\")\n",
    "    model.eval()\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=device)\n",
    "    \n",
    "    for i in range(max_length):\n",
    "        print(f\"  Generating token {i+1}/{max_length}\")\n",
    "        src_key_padding_mask = (~attention_mask.bool()).transpose(0, 1)\n",
    "        tgt_key_padding_mask = (~attention_mask.bool()).transpose(0, 1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(input_ids, input_ids, src_mask=src_key_padding_mask, tgt_mask=tgt_key_padding_mask)\n",
    "        \n",
    "        next_token_logits = output[0, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "        attention_mask = torch.cat([attention_mask, torch.ones((1, 1), dtype=torch.long, device=device)], dim=1)\n",
    "        \n",
    "        if next_token.item() == tokenizer.sep_token_id:\n",
    "            print(\"  Reached end of sequence token\")\n",
    "            break\n",
    "    \n",
    "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "    print(\"Text generation completed\")\n",
    "    return generated_text\n",
    "\n",
    "# Run a prompt\n",
    "print(\"\\nStep 4: Running a prompt\")\n",
    "prompt = \"The\"\n",
    "print(f\"Prompt: {prompt}\")\n",
    "generated_text = generate_text(prompt)\n",
    "print(f\"Generated text: {generated_text}\")\n",
    "\n",
    "print(\"\\nStep 5: Process completed\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
