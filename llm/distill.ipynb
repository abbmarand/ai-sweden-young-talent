{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/m/dev/ai/lib/python3.12/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:15<00:00,  7.59s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "\n",
    "model_name = \"./model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"cpu\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    device_map=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, how are you?\n",
      "Hello, I'm just a computer program, so I don't have feelings or emotions like a human does. I'm here to help you with any questions you have or provide information on a topic you're interested in. Is there something specific you'd like to know about?\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Hello, how are you?\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        num_return_sequences=1,\n",
    "    )\n",
    "\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "%pip install -U bitsandbytes\n",
    "%pip install -U transformers\n",
    "%pip install -U peft\n",
    "%pip install -U accelerate\n",
    "%pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'is_grokadamw_available' from 'transformers.utils' (/home/m/dev/ai/lib/python3.12/site-packages/transformers/utils/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/transformers/utils/import_utils.py:1603\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[1;32m   1560\u001b[0m     [\n\u001b[1;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[1;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[1;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[1;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[1;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[1;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[1;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[1;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[1;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[1;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[1;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[1;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[1;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[1;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[1;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[1;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[1;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[1;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[1;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[1;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[1;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[1;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[1;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[1;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[1;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[1;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[1;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[1;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[1;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[1;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[1;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[1;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[1;32m   1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[1;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[1;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[1;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[1;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[1;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[1;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[1;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[1;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[1;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[0;32m-> 1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[1;32m   1604\u001b[0m     ]\n\u001b[1;32m   1605\u001b[0m )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/transformers/trainer.py:136\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining_args\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OptimizerNames, ParallelMode, TrainingArguments\n\u001b[0;32m--> 136\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    137\u001b[0m     ADAPTER_CONFIG_NAME,\n\u001b[1;32m    138\u001b[0m     ADAPTER_SAFE_WEIGHTS_NAME,\n\u001b[1;32m    139\u001b[0m     ADAPTER_WEIGHTS_NAME,\n\u001b[1;32m    140\u001b[0m     CONFIG_NAME,\n\u001b[1;32m    141\u001b[0m     SAFE_WEIGHTS_INDEX_NAME,\n\u001b[1;32m    142\u001b[0m     SAFE_WEIGHTS_NAME,\n\u001b[1;32m    143\u001b[0m     WEIGHTS_INDEX_NAME,\n\u001b[1;32m    144\u001b[0m     WEIGHTS_NAME,\n\u001b[1;32m    145\u001b[0m     XLA_FSDPV2_MIN_VERSION,\n\u001b[1;32m    146\u001b[0m     PushInProgress,\n\u001b[1;32m    147\u001b[0m     PushToHubMixin,\n\u001b[1;32m    148\u001b[0m     can_return_loss,\n\u001b[1;32m    149\u001b[0m     find_labels,\n\u001b[1;32m    150\u001b[0m     is_accelerate_available,\n\u001b[1;32m    151\u001b[0m     is_apex_available,\n\u001b[1;32m    152\u001b[0m     is_bitsandbytes_available,\n\u001b[1;32m    153\u001b[0m     is_datasets_available,\n\u001b[1;32m    154\u001b[0m     is_galore_torch_available,\n\u001b[1;32m    155\u001b[0m     is_grokadamw_available,\n\u001b[1;32m    156\u001b[0m     is_in_notebook,\n\u001b[1;32m    157\u001b[0m     is_ipex_available,\n\u001b[1;32m    158\u001b[0m     is_liger_kernel_available,\n\u001b[1;32m    159\u001b[0m     is_lomo_available,\n\u001b[1;32m    160\u001b[0m     is_peft_available,\n\u001b[1;32m    161\u001b[0m     is_safetensors_available,\n\u001b[1;32m    162\u001b[0m     is_sagemaker_dp_enabled,\n\u001b[1;32m    163\u001b[0m     is_sagemaker_mp_enabled,\n\u001b[1;32m    164\u001b[0m     is_schedulefree_available,\n\u001b[1;32m    165\u001b[0m     is_torch_compile_available,\n\u001b[1;32m    166\u001b[0m     is_torch_mlu_available,\n\u001b[1;32m    167\u001b[0m     is_torch_mps_available,\n\u001b[1;32m    168\u001b[0m     is_torch_musa_available,\n\u001b[1;32m    169\u001b[0m     is_torch_neuroncore_available,\n\u001b[1;32m    170\u001b[0m     is_torch_npu_available,\n\u001b[1;32m    171\u001b[0m     is_torch_xla_available,\n\u001b[1;32m    172\u001b[0m     is_torch_xpu_available,\n\u001b[1;32m    173\u001b[0m     is_torchao_available,\n\u001b[1;32m    174\u001b[0m     logging,\n\u001b[1;32m    175\u001b[0m     strtobool,\n\u001b[1;32m    176\u001b[0m )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mquantization_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QuantizationMethod\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'is_grokadamw_available' from 'transformers.utils' (/home/m/dev/ai/lib/python3.12/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/trl/import_utils.py:142\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3.12/importlib/__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:28\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_deprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _deprecate_arguments\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m     30\u001b[0m     AutoTokenizer,\n\u001b[1;32m     31\u001b[0m     DataCollator,\n\u001b[1;32m     32\u001b[0m     DataCollatorForLanguageModeling,\n\u001b[1;32m     33\u001b[0m     PreTrainedModel,\n\u001b[1;32m     34\u001b[0m     PreTrainedTokenizerBase,\n\u001b[1;32m     35\u001b[0m     Trainer,\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unwrap_model\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/transformers/utils/import_utils.py:1593\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[1;32m   1560\u001b[0m     [\n\u001b[1;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[1;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[1;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[1;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[1;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[1;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[1;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[1;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[1;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[1;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[1;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[1;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[1;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[1;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[1;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[1;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[1;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[1;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[1;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[1;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[1;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[1;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[1;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[1;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[1;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[1;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[1;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[1;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[1;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[1;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[1;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[1;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[0;32m-> 1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[1;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[1;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[1;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[1;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[1;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[1;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[1;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[1;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[1;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[1;32m   1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[1;32m   1604\u001b[0m     ]\n\u001b[1;32m   1605\u001b[0m )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/transformers/utils/import_utils.py:1605\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1554\u001b[0m JINJA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jinja library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;124mjinja2`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1559\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[1;32m   1560\u001b[0m     [\n\u001b[1;32m   1561\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mav\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_av_available, AV_IMPORT_ERROR)),\n\u001b[1;32m   1562\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[1;32m   1563\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcv2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cv2_available, CV2_IMPORT_ERROR)),\n\u001b[1;32m   1564\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[1;32m   1565\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[1;32m   1566\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[1;32m   1567\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[1;32m   1568\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[1;32m   1569\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[1;32m   1570\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mg2p_en\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_g2p_en_available, G2P_EN_IMPORT_ERROR)),\n\u001b[1;32m   1571\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[1;32m   1572\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[1;32m   1573\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muroman\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_uroman_available, UROMAN_IMPORT_ERROR)),\n\u001b[1;32m   1574\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[1;32m   1575\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevenshtein\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_levenshtein_available, LEVENSHTEIN_IMPORT_ERROR)),\n\u001b[1;32m   1576\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[1;32m   1577\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[1;32m   1578\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[1;32m   1579\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[1;32m   1580\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[1;32m   1581\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[1;32m   1582\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[1;32m   1583\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[1;32m   1584\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[1;32m   1585\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[1;32m   1586\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[1;32m   1587\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[1;32m   1588\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[1;32m   1589\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchaudio\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchaudio_available, TORCHAUDIO_IMPORT_ERROR)),\n\u001b[1;32m   1590\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[1;32m   1591\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnltk\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_nltk_available, NLTK_IMPORT_ERROR)),\n\u001b[1;32m   1592\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[1;32m   1593\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[1;32m   1594\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[1;32m   1595\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[1;32m   1596\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[1;32m   1597\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[1;32m   1598\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[1;32m   1599\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[1;32m   1600\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[1;32m   1601\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[1;32m   1602\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[1;32m   1603\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjinja\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jinja_available, JINJA_IMPORT_ERROR)),\n\u001b[1;32m   1604\u001b[0m     ]\n\u001b[0;32m-> 1605\u001b[0m )\n\u001b[1;32m   1608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'is_grokadamw_available' from 'transformers.utils' (/home/m/dev/ai/lib/python3.12/site-packages/transformers/utils/__init__.py)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtrl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkaggle_secrets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserSecretsClient\n\u001b[1;32m      7\u001b[0m user_secrets \u001b[38;5;241m=\u001b[39m UserSecretsClient()\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/trl/import_utils.py:133\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    132\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[0;32m--> 133\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/trl/import_utils.py:132\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    130\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m--> 132\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/dev/ai/lib/python3.12/site-packages/trl/import_utils.py:144\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import trl.trainer.sft_trainer because of the following error (look up to see its traceback):\nFailed to import transformers.trainer because of the following error (look up to see its traceback):\ncannot import name 'is_grokadamw_available' from 'transformers.utils' (/home/m/dev/ai/lib/python3.12/site-packages/transformers/utils/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,HfArgumentParser,TrainingArguments,pipeline, logging\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "import os,torch, wandb\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "secret_hf = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\n",
    "secret_wandb = user_secrets.get_secret(\"wandb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.add_bos_token, tokenizer.add_eos_token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
